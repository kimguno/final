{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "# UserWarning 무시 (필요 시 제거 가능)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# 디바이스 설정 (GPU 사용 여부 확인)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# transformer 모듈 정의 (트랜스포머 기반 시계열 처리 모델)\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, seq_length, embed_dim=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_linear = nn.Linear(input_dim, embed_dim)\n",
    "        \n",
    "        # TransformerEncoderLayer에 batch_first=True 설정\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  # batch_first 설정\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_length, input_dim]\n",
    "        \"\"\"\n",
    "        x = self.input_linear(x)  # [batch_size, seq_length, embed_dim]\n",
    "        x = self.transformer_encoder(x)  # [batch_size, seq_length, embed_dim]\n",
    "        x = self.output_linear(x)  # [batch_size, seq_length, embed_dim]\n",
    "        x = self.activation(x)\n",
    "        # 시퀀스의 마지막 타임스탬프 출력\n",
    "        x = x[:, -1, :]           # [batch_size, embed_dim]\n",
    "        return x\n",
    "\n",
    "# 다중 종목 주식 트레이딩 환경 정의\n",
    "class MultiStockTradingEnv(gym.Env):\n",
    "    def __init__(self, dfs, stock_dim=3, initial_balance=1000000, max_stock=100, seq_length=20):\n",
    "        super(MultiStockTradingEnv, self).__init__()\n",
    "        self.dfs = dfs  # 종목별 데이터프레임 딕셔너리\n",
    "        self.stock_dim = stock_dim  # 종목 수\n",
    "        self.initial_balance = initial_balance  # 초기 자본금\n",
    "        self.max_stock = max_stock  # 각 종목당 최대 보유 주식 수\n",
    "        self.transaction_fee = 0.0025  # 거래 수수료 0.25%\n",
    "        self.slippage = 0.003  # 슬리피지 0.3%\n",
    "        self.max_loss = 0.2  # 최대 허용 손실 (20%)\n",
    "        self.seq_length = seq_length  # 시퀀스 길이\n",
    "\n",
    "        # 상태 공간: 시퀀스 길이에 따른 각 종목의 [Close, MA10, MA20, RSI]와 보유 주식 수\n",
    "        # plus balance\n",
    "        # 각 종목: (4 * seq_length) + 1 (보유 주식 수)\n",
    "        # 총: stock_dim * (4 * seq_length +1) + 1 (balance)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.stock_dim * (4 * self.seq_length + 1) + 1,), dtype=np.float32\n",
    "        )\n",
    "        self.action_space = spaces.MultiDiscrete([9] * self.stock_dim)  # 각 종목별로 9개의 액션\n",
    "\n",
    "        # 초기화\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.current_step = 0\n",
    "        self.stock_owned = {ticker: {'quantity': 0, 'avg_price': 0} for ticker in self.dfs.keys()}\n",
    "        self.stock_price = {}\n",
    "        self.total_asset = []\n",
    "\n",
    "        # 히스토리 초기화\n",
    "        self.balance_history = [self.balance]\n",
    "        self.portfolio_value_history = [self.portfolio_value]\n",
    "        self.action_history = []\n",
    "        self.price_history = {ticker: [] for ticker in self.dfs.keys()}\n",
    "        self.trade_history = []  # 매수/매도 내역 저장\n",
    "\n",
    "        # 각 종목의 최대 스텝 수 계산\n",
    "        self.max_steps = min(len(df) for df in self.dfs.values()) - self.seq_length - 1\n",
    "        if self.max_steps <= 0:\n",
    "            raise ValueError(\"데이터프레임의 길이가 시퀀스 길이보다 짧습니다.\")\n",
    "\n",
    "        # 각 종목의 현재 인덱스 초기화\n",
    "        self.data_indices = {ticker: self.seq_length for ticker in self.dfs.keys()}  # 시퀀스 길이만큼 초기 인덱스\n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = []\n",
    "        for ticker, df in self.dfs.items():\n",
    "            idx = self.data_indices[ticker]\n",
    "            if idx < self.seq_length:\n",
    "                # 충분한 시퀀스가 없으면 제로 패딩\n",
    "                seq = df.loc[:idx, ['Close', 'MA10', 'MA20', 'RSI']].values\n",
    "                pad_length = self.seq_length - seq.shape[0]\n",
    "                if pad_length > 0:\n",
    "                    pad = np.zeros((pad_length, 4))\n",
    "                    seq = np.vstack((pad, seq))\n",
    "            else:\n",
    "                # 시퀀스 슬라이싱\n",
    "                seq = df.loc[idx - self.seq_length:idx - 1, ['Close', 'MA10', 'MA20', 'RSI']].values\n",
    "            obs.extend(seq.flatten())  # [seq_length * 4]\n",
    "            obs.append(self.stock_owned[ticker]['quantity'])  # [1]\n",
    "\n",
    "            # 현재 가격 저장 (iloc 사용)\n",
    "            self.stock_price[ticker] = df.iloc[idx]['Close_unscaled']\n",
    "\n",
    "        # 잔고 추가\n",
    "        obs.append(self.balance)  # [1]\n",
    "\n",
    "        return np.array(obs, dtype=np.float32)\n",
    "\n",
    "    def step(self, actions):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        invalid_action_penalty = -10  # 불가능한 행동에 대한 페널티\n",
    "        self.action_history.append(actions)\n",
    "        trade_info = []  # 현재 스텝의 거래 내역\n",
    "\n",
    "        prev_portfolio_value = self.portfolio_value  # 이전 포트폴리오 가치 저장\n",
    "\n",
    "        # 액션 맵핑 정의\n",
    "        action_mapping = {\n",
    "            0: ('sell', 1.0),\n",
    "            1: ('sell', 0.75),\n",
    "            2: ('sell', 0.5),\n",
    "            3: ('sell', 0.25),\n",
    "            4: ('hold', 0.0),\n",
    "            5: ('buy', 0.25),\n",
    "            6: ('buy', 0.5),\n",
    "            7: ('buy', 0.75),\n",
    "            8: ('buy', 1.0)\n",
    "        }\n",
    "\n",
    "        # 종목별로 행동 수행\n",
    "        for i, (ticker, df) in enumerate(self.dfs.items()):\n",
    "            action = actions[i]\n",
    "            idx = self.data_indices[ticker]\n",
    "\n",
    "            # 인덱스가 데이터프레임의 범위를 벗어나면 에피소드를 종료\n",
    "            if idx >= len(df):\n",
    "                done = True\n",
    "                trade_info.append(f\"Ticker {ticker} reached end of data. Ending episode.\")\n",
    "                break\n",
    "\n",
    "            actual_price = df.iloc[idx]['Close_unscaled']\n",
    "\n",
    "            # 액션 맵핑을 통해 행동 타입과 비율 얻기\n",
    "            action_type, proportion = action_mapping.get(action, ('hold', 0.0))\n",
    "\n",
    "            # 슬리피지 적용\n",
    "            if action_type == 'buy':\n",
    "                adjusted_price = actual_price * (1 + self.slippage)\n",
    "            elif action_type == 'sell':\n",
    "                adjusted_price = actual_price * (1 - self.slippage)\n",
    "            else:\n",
    "                adjusted_price = actual_price\n",
    "\n",
    "            # 거래 수수료 계산\n",
    "            fee = adjusted_price * self.transaction_fee\n",
    "\n",
    "            # buy_amount과 sell_amount 초기화\n",
    "            buy_amount = 0\n",
    "            sell_amount = 0\n",
    "\n",
    "            # 행동에 따른 포트폴리오 업데이트 및 보상 계산\n",
    "            reward = 0  # 각 종목별 보상 초기화\n",
    "            if action_type == 'sell':\n",
    "                if self.stock_owned[ticker]['quantity'] > 0:\n",
    "                    sell_amount = int(self.stock_owned[ticker]['quantity'] * proportion)\n",
    "                    sell_amount = max(1, sell_amount)  # 최소 1주 매도\n",
    "                    sell_amount = min(sell_amount, self.stock_owned[ticker]['quantity'])  # 보유 주식 수 초과 방지\n",
    "                    proceeds = adjusted_price * sell_amount - fee * sell_amount\n",
    "                    self.balance += proceeds\n",
    "                    # 이익 또는 손실 계산\n",
    "                    profit = (adjusted_price - self.stock_owned[ticker]['avg_price']) * sell_amount - fee * sell_amount\n",
    "                    reward = profit  # 매도 시 보상은 이익 또는 손실\n",
    "                    self.stock_owned[ticker]['quantity'] -= sell_amount\n",
    "                    if self.stock_owned[ticker]['quantity'] == 0:\n",
    "                        self.stock_owned[ticker]['avg_price'] = 0\n",
    "                    trade_info.append(f\"Sell {sell_amount} of {ticker} at {adjusted_price:.2f}\")\n",
    "                else:\n",
    "                    # 보유한 주식이 없으면 페널티 부여\n",
    "                    reward = invalid_action_penalty\n",
    "                    trade_info.append(f\"Cannot Sell {ticker} (No holdings)\")\n",
    "            elif action_type == 'buy':\n",
    "                max_can_buy = min(\n",
    "                    self.max_stock - self.stock_owned[ticker]['quantity'],\n",
    "                    int(self.balance // (adjusted_price + fee))\n",
    "                )\n",
    "                buy_amount = int(max_can_buy * proportion)\n",
    "                buy_amount = max(1, buy_amount)  # 최소 1주 매수\n",
    "                buy_amount = min(buy_amount, self.max_stock - self.stock_owned[ticker]['quantity'], \n",
    "                                 int(self.balance // (adjusted_price + fee)))\n",
    "                if buy_amount > 0:\n",
    "                    cost = adjusted_price * buy_amount + fee * buy_amount\n",
    "                    self.balance -= cost\n",
    "                    # 평균 매수가격 업데이트\n",
    "                    total_quantity = self.stock_owned[ticker]['quantity'] + buy_amount\n",
    "                    if total_quantity > 0:\n",
    "                        self.stock_owned[ticker]['avg_price'] = (\n",
    "                            (self.stock_owned[ticker]['avg_price'] * self.stock_owned[ticker]['quantity'] + adjusted_price * buy_amount)\n",
    "                            / total_quantity\n",
    "                        )\n",
    "                    self.stock_owned[ticker]['quantity'] = total_quantity\n",
    "                    reward = 0  # 매수 시 보상 없음\n",
    "                    trade_info.append(f\"Buy {buy_amount} of {ticker} at {adjusted_price:.2f}\")\n",
    "                else:\n",
    "                    reward = invalid_action_penalty  # 매수 불가 시 페널티\n",
    "                    trade_info.append(f\"Cannot Buy {ticker} (Insufficient balance or max stock)\")\n",
    "            else:\n",
    "                trade_info.append(f\"Hold {ticker}\")\n",
    "\n",
    "            # 보상 누적\n",
    "            total_reward += reward\n",
    "\n",
    "            # 가격 히스토리 업데이트\n",
    "            self.price_history[ticker].append(actual_price)\n",
    "\n",
    "            # 다음 인덱스로 이동\n",
    "            self.data_indices[ticker] += 1\n",
    "\n",
    "            # 디버깅을 위한 로그 출력\n",
    "            print(f\"Ticker: {ticker}, Action: {action_type}, Proportion: {proportion}, \"\n",
    "                  f\"Buy/Sell Amount: {buy_amount if action_type == 'buy' else sell_amount if action_type == 'sell' else 0}, \"\n",
    "                  f\"Balance: {self.balance:.2f}, Holdings: {self.stock_owned[ticker]['quantity']}\")\n",
    "\n",
    "            # 잔고와 보유 주식 수량이 음수가 되지 않도록 방지\n",
    "            if self.balance < 0:\n",
    "                print(f\"Warning: Balance for {ticker} is negative! Setting to 0.\")\n",
    "                self.balance = 0\n",
    "            if self.stock_owned[ticker]['quantity'] < 0:\n",
    "                print(f\"Warning: Holdings for {ticker} are negative! Setting to 0.\")\n",
    "                self.stock_owned[ticker]['quantity'] = 0\n",
    "                self.stock_owned[ticker]['avg_price'] = 0\n",
    "\n",
    "        # 현재 스텝의 거래 내역 저장 및 출력\n",
    "        if trade_info:\n",
    "            print(f\"Episode {self.current_step} Step {self.current_step}:\")\n",
    "            for info in trade_info:\n",
    "                print(info)\n",
    "            print()\n",
    "\n",
    "        self.trade_history.append(trade_info)\n",
    "\n",
    "        # 현재 포트폴리오 가치 계산\n",
    "        self.portfolio_value = self.balance + sum(\n",
    "            self.stock_owned[ticker]['quantity'] * self.stock_price[ticker] for ticker in self.dfs.keys()\n",
    "        )\n",
    "        self.total_asset.append(self.portfolio_value)\n",
    "\n",
    "        # 보상은 포트폴리오 가치의 비율적 변화량을 사용\n",
    "        if prev_portfolio_value > 0:\n",
    "            portfolio_return = (self.portfolio_value - prev_portfolio_value) / prev_portfolio_value  # 비율적 변화\n",
    "        else:\n",
    "            portfolio_return = 0  # 이전 포트폴리오 가치가 0일 경우\n",
    "        scaled_reward = portfolio_return * 100  # 예: 0.01 -> 1%\n",
    "        total_reward += scaled_reward  # 전체 보상에 추가\n",
    "\n",
    "        # 포트폴리오 가치가 음수가 되지 않도록 보장\n",
    "        if self.portfolio_value < 0:\n",
    "            print(f\"Error: Portfolio value is negative! Setting to 0.\")\n",
    "            self.portfolio_value = 0\n",
    "            self.balance = 0  # 잔고도 0으로 설정\n",
    "            done = True  # 에피소드를 종료\n",
    "\n",
    "        # 최대 허용 손실 초과 시 종료\n",
    "        if self.portfolio_value < self.initial_balance * (1 - self.max_loss):\n",
    "            done = True\n",
    "\n",
    "        # 최대 스텝 수 초과 시 종료\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        # 히스토리 업데이트\n",
    "        self.balance_history.append(self.balance)\n",
    "        self.portfolio_value_history.append(self.portfolio_value)\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, total_reward, done, {}\n",
    "\n",
    "# 데이터 로드 및 기술적 지표 계산\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL']\n",
    "dfs = {}\n",
    "for ticker in tickers:\n",
    "    df = yf.download(ticker, start='2020-01-01', end='2020-12-31', progress=False)\n",
    "\n",
    "    # 원래의 Close 가격 보존\n",
    "    df.loc[:, 'Close_unscaled'] = df['Close']  # 실제 가격 저장\n",
    "\n",
    "    # 기술적 지표 계산\n",
    "    df.loc[:, 'MA10'] = df['Close'].rolling(window=10).mean()\n",
    "    df.loc[:, 'MA20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "    # RSI 계산\n",
    "    delta = df['Close'].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    ema_up = up.ewm(com=13, adjust=False).mean()\n",
    "    ema_down = down.ewm(com=13, adjust=False).mean()\n",
    "    rs = ema_up / ema_down\n",
    "    df.loc[:, 'RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    df = df.dropna().copy()  # 복사하여 경고 방지\n",
    "\n",
    "    # 입력 데이터 정규화\n",
    "    scaler = StandardScaler()\n",
    "    feature_cols = ['Close', 'MA10', 'MA20', 'RSI']\n",
    "    df.loc[:, feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "\n",
    "    dfs[ticker] = df.reset_index(drop=True)\n",
    "\n",
    "# 환경 생성 (초기 자본금 증가 및 시퀀스 길이 설정)\n",
    "env = MultiStockTradingEnv(dfs, initial_balance=1000000, seq_length=20)\n",
    "\n",
    "# PPO를 위한 액터-크리틱 신경망 정의\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim_list, seq_length=20):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.transformer = Transformer(input_dim=4, seq_length=seq_length).to(device)  # input_dim=4 (Close, MA10, MA20, RSI)\n",
    "        self.policy_head = nn.ModuleList([nn.Linear(self.transformer.embed_dim, action_dim) for action_dim in action_dim_list])\n",
    "        self.value_head = nn.Linear(self.transformer.embed_dim * len(action_dim_list), 1)  # embed_dim * stock_dim\n",
    "        self.apply(self._weights_init)  # 가중치 초기화\n",
    "\n",
    "    def _weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # 정책 헤드의 바이어스를 0으로 초기화하여 행동 확률 균등화\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, stock_dim * (4 * seq_length +1) +1]\n",
    "        \"\"\"\n",
    "        stock_embeds = []\n",
    "        for i, ticker in enumerate(tickers):\n",
    "            # 각 종목의 시퀀스 데이터: [batch_size, 4 * seq_length]\n",
    "            start = i * (4 * self.seq_length + 1)\n",
    "            end = start + 4 * self.seq_length\n",
    "            seq = x[:, start:end]\n",
    "            # Reshape to [batch_size, seq_length, 4]\n",
    "            seq = seq.view(-1, self.seq_length, 4)\n",
    "            embed = self.transformer(seq)  # [batch_size, embed_dim]\n",
    "            stock_embeds.append(embed)\n",
    "        # 정책 헤드는 각 embed를 처리하여 policy logits을 생성\n",
    "        policy_logits = [head(embed) for embed, head in zip(stock_embeds, self.policy_head)]  # [batch_size, 9] * stock_dim\n",
    "        # 가치 함수는 모든 embed를 합쳐 처리\n",
    "        combined_embeds = torch.cat(stock_embeds, dim=1)  # [batch_size, embed_dim * stock_dim]\n",
    "        value = self.value_head(combined_embeds)  # [batch_size, 1]\n",
    "        return policy_logits, value\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.to(device)\n",
    "        policy_logits, _ = self.forward(state)\n",
    "        actions = []\n",
    "        action_logprobs = []\n",
    "        for logits in policy_logits:\n",
    "            dist = Categorical(logits=logits)\n",
    "            action = dist.sample()\n",
    "            actions.append(action.item())\n",
    "            action_logprob = dist.log_prob(action)\n",
    "            action_logprobs.append(action_logprob)\n",
    "        return np.array(actions), torch.stack(action_logprobs)\n",
    "\n",
    "    def evaluate(self, state, actions):\n",
    "        policy_logits, value = self.forward(state)\n",
    "        action_logprobs = []\n",
    "        dist_entropies = []\n",
    "        for i, logits in enumerate(policy_logits):\n",
    "            dist = Categorical(logits=logits)\n",
    "            action_logprob = dist.log_prob(actions[:, i])\n",
    "            dist_entropy = dist.entropy()\n",
    "            action_logprobs.append(action_logprob)\n",
    "            dist_entropies.append(dist_entropy)\n",
    "        return torch.stack(action_logprobs, dim=1), value.squeeze(-1), torch.stack(dist_entropies, dim=1)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 1e-4  # 학습률\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "epochs = 10  # 에폭 수\n",
    "\n",
    "# 정책 및 옵티마이저 초기화\n",
    "input_dim = env.observation_space.shape[0]\n",
    "action_dim_list = [9] * env.stock_dim  # 각 종목별 9개의 액션\n",
    "policy = ActorCritic(input_dim, action_dim_list, seq_length=env.seq_length).to(device)  # 모델을 GPU로 이동\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "# 메모리 클래스 정의\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "memory = Memory()\n",
    "\n",
    "# PPO 업데이트 함수 정의\n",
    "def ppo_update():\n",
    "    if len(memory.states) == 0:\n",
    "        return  # 메모리가 비어있으면 업데이트하지 않음\n",
    "\n",
    "    # 리스트를 텐서로 변환하고 GPU로 이동\n",
    "    states = torch.tensor(memory.states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(memory.actions, dtype=torch.int64).to(device)\n",
    "    old_logprobs = torch.tensor(memory.logprobs, dtype=torch.float32).to(device)\n",
    "    rewards = memory.rewards\n",
    "    is_terminals = memory.is_terminals\n",
    "\n",
    "    # 리턴 계산 (할인 보상)\n",
    "    returns = []\n",
    "    discounted_reward = 0\n",
    "    for reward, is_terminal in zip(reversed(rewards), reversed(is_terminals)):\n",
    "        if is_terminal:\n",
    "            discounted_reward = 0\n",
    "        discounted_reward = reward + (gamma * discounted_reward)\n",
    "        returns.insert(0, discounted_reward)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 어드밴티지 계산\n",
    "    with torch.no_grad():\n",
    "        _, state_values = policy.forward(states)\n",
    "        advantages = returns - state_values.squeeze(-1)\n",
    "        # 어드밴티지 정규화\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # 정책 업데이트\n",
    "    for epoch in range(epochs):\n",
    "        action_logprobs, state_values, dist_entropies = policy.evaluate(states, actions)\n",
    "        # 정책 헤드별 로그 확률 합계\n",
    "        total_logprobs = action_logprobs.sum(dim=1)\n",
    "        # 정책 헤드별 엔트로피 합계\n",
    "        total_entropies = dist_entropies.sum(dim=1)\n",
    "        ratios = torch.exp(total_logprobs - old_logprobs)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "        # 가치 함수 손실 추가\n",
    "        value_loss = F.mse_loss(state_values.squeeze(-1), returns)\n",
    "        entropy_coef = 0.01  # 엔트로피 계수 감소\n",
    "        loss = -torch.min(surr1, surr2) + 0.5 * value_loss - entropy_coef * total_entropies\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_mean = loss.mean()\n",
    "        loss_mean.backward()\n",
    "\n",
    "        # 그레이디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 값 출력\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss_mean.item():.4f}\")\n",
    "\n",
    "# 학습 루프\n",
    "max_episodes = 100  # 에피소드 수\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)  # [1, state_dim]\n",
    "        actions, action_logprobs = policy.act(state_tensor)\n",
    "        actions = actions  # [stock_dim]\n",
    "        next_state, reward, done, _ = env.step(actions)\n",
    "        total_reward += reward\n",
    "        # 메모리에 데이터 저장\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(actions)\n",
    "        memory.logprobs.append(action_logprobs.sum().item())\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "        state = next_state\n",
    "\n",
    "    # 정책 업데이트 및 메모리 초기화\n",
    "    ppo_update()\n",
    "    memory.clear()\n",
    "\n",
    "    # 행동 분포 시각화\n",
    "    action_counts = np.zeros(9)\n",
    "    for actions in env.action_history:\n",
    "        action_counts += np.bincount(actions, minlength=9)\n",
    "    plt.figure(figsize=(18, 12))\n",
    "\n",
    "    # 포트폴리오 가치 변화 시각화\n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(env.portfolio_value_history)\n",
    "    plt.title(f'Episode {episode+1} - Portfolio Value Over Time')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "\n",
    "    # 행동 분포 시각화\n",
    "    plt.subplot(4, 1, 2)\n",
    "    action_labels = ['Sell 100%', 'Sell 75%', 'Sell 50%', 'Sell 25%', 'Hold', \n",
    "                    'Buy 25%', 'Buy 50%', 'Buy 75%', 'Buy 100%']\n",
    "    plt.bar(action_labels, action_counts, color=['red', 'darkred', 'orange', 'lightcoral', 'gray', \n",
    "                                                'lightgreen', 'green', 'darkgreen', 'lime'])\n",
    "    plt.title('Action Distribution')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # 종목별 수익률 시각화\n",
    "    plt.subplot(4, 1, 3)\n",
    "    initial_prices = {ticker: env.price_history[ticker][0] for ticker in tickers}\n",
    "    final_prices = {ticker: env.price_history[ticker][-1] for ticker in tickers}\n",
    "    returns = []\n",
    "    for ticker in tickers:\n",
    "        if initial_prices[ticker] == 0:\n",
    "            ret = 0\n",
    "        else:\n",
    "            ret = (final_prices[ticker] - initial_prices[ticker]) / initial_prices[ticker] * 100\n",
    "        returns.append(ret)\n",
    "    plt.bar(tickers, returns, color=['blue', 'orange', 'purple'])\n",
    "    plt.title('Stock Returns (%)')\n",
    "    plt.ylabel('Return (%)')\n",
    "\n",
    "    # 종목별 보유 주식 수 시각화\n",
    "    plt.subplot(4, 1, 4)\n",
    "    hold_counts = [env.stock_owned[ticker]['quantity'] for ticker in tickers]\n",
    "    plt.bar(tickers, hold_counts, color=['cyan', 'magenta', 'yellow'])\n",
    "    plt.title('Current Holdings')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Episode {episode+1} completed. Total Reward: {total_reward:.2f}, Final Portfolio Value: {env.portfolio_value_history[-1]:.2f}\")\n",
    "    print(f\"Action counts:\")\n",
    "    for i, label in enumerate(action_labels):\n",
    "        print(f\"  {label}: {int(action_counts[i])}\")\n",
    "    print('-' * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
