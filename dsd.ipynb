{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "강화학습 PPO 알고리즘 적용한 예제 코드입니다\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터 로드 및 기술적 지표 계산\n",
    "df = yf.download('AAPL', start='2017-01-01', end='2020-12-31', progress=False)\n",
    "df['MA10'] = df['Close'].rolling(window=10).mean()\n",
    "df['MA50'] = df['Close'].rolling(window=50).mean()\n",
    "df['RSI'] = 100 - (100 / (1 + df['Close'].diff().apply(lambda x: np.maximum(x, 0))\n",
    "                          .rolling(window=14).mean() / df['Close'].diff()\n",
    "                          .apply(lambda x: np.abs(np.minimum(x, 0)))\n",
    "                          .rolling(window=14).mean()))\n",
    "df = df.dropna().reset_index()\n",
    "\n",
    "# 입력 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "feature_cols = ['Close', 'MA10', 'MA50', 'RSI']\n",
    "scaler.fit(df[feature_cols])\n",
    "\n",
    "# 주식 트레이딩 환경 정의\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df, scaler):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.scaler = scaler\n",
    "        self.max_steps = len(df) - 1\n",
    "        self.current_step = 0\n",
    "\n",
    "        # 상태 공간: [현재 가격, MA10, MA50, RSI, 보유 여부]\n",
    "        self.action_space = spaces.Discrete(3)  # 매도(0), 보유(1), 매수(2)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # 초기 자본금 및 포지션\n",
    "        self.initial_balance = 10000\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0  # 0: 현금, 1: 주식 보유\n",
    "        self.asset_value = self.initial_balance  # 포트폴리오 총 가치\n",
    "\n",
    "        # 추적을 위한 리스트\n",
    "        self.balance_history = []\n",
    "        self.asset_value_history = []\n",
    "        self.position_history = []\n",
    "        self.action_history = []\n",
    "        self.price_history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0\n",
    "        self.asset_value = self.initial_balance\n",
    "        self.current_step = 0\n",
    "\n",
    "        # 히스토리 초기화\n",
    "        self.balance_history = [self.balance]\n",
    "        self.asset_value_history = [self.asset_value]\n",
    "        self.position_history = [self.position]\n",
    "        self.action_history = []\n",
    "        self.price_history = [self.df.loc[self.current_step, 'Close']]\n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        raw_state = self.df.loc[self.current_step, ['Close', 'MA10', 'MA50', 'RSI']]\n",
    "        # Series를 DataFrame으로 변환\n",
    "        raw_state_df = pd.DataFrame([raw_state])\n",
    "        # scaler.transform에 DataFrame을 입력\n",
    "        scaled_state = self.scaler.transform(raw_state_df)[0]\n",
    "        frame = np.append(scaled_state, self.position)\n",
    "        return frame.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        price = self.df.loc[self.current_step, 'Close']\n",
    "\n",
    "        # 이전 포트폴리오 가치\n",
    "        prev_asset_value = self.asset_value\n",
    "\n",
    "        # 행동에 따른 포트폴리오 업데이트\n",
    "        if action == 0:  # 매도\n",
    "            if self.position == 1:\n",
    "                self.balance += price\n",
    "                self.position = 0\n",
    "        elif action == 1:  # 보유\n",
    "            pass\n",
    "        elif action == 2:  # 매수\n",
    "            if self.position == 0:\n",
    "                self.balance -= price\n",
    "                self.position = 1\n",
    "\n",
    "        # 포트폴리오 가치 계산\n",
    "        self.asset_value = self.balance + self.position * price\n",
    "\n",
    "        # 보유 비용 적용 (포지션을 보유할 때마다 일정 비용 부과)\n",
    "        holding_cost = 0\n",
    "        if self.position == 1:\n",
    "            holding_cost = 0.001 * price  # 가격의 0.1%를 보유 비용으로 설정\n",
    "            self.asset_value -= holding_cost\n",
    "\n",
    "        # 보상 계산: 포트폴리오 가치의 변화 (스케일링)\n",
    "        reward = (self.asset_value - prev_asset_value) * 10  # 보상 스케일링\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        # 히스토리 업데이트\n",
    "        self.balance_history.append(self.balance)\n",
    "        self.asset_value_history.append(self.asset_value)\n",
    "        self.position_history.append(self.position)\n",
    "        self.action_history.append(action)\n",
    "        self.price_history.append(price)\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# 환경 생성\n",
    "env = StockTradingEnv(df, scaler)\n",
    "\n",
    "# PPO를 위한 액터-크리틱 신경망 정의\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(128, action_dim)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        policy_logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return policy_logits, value\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        policy_logits, _ = self.forward(state)\n",
    "        dist = Categorical(logits=policy_logits)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        return action.item(), action_logprob\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        policy_logits, value = self.forward(state)\n",
    "        dist = Categorical(logits=policy_logits)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        return action_logprobs, value.squeeze(-1), dist_entropy\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 1e-3  # 학습률 증가\n",
    "gamma = 0.99\n",
    "epsilon = 0.1         # 클리핑 파라미터 감소\n",
    "epochs = 10\n",
    "entropy_coef = 0.05   # 엔트로피 가중치 증가\n",
    "\n",
    "# 정책 및 옵티마이저 초기화\n",
    "input_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "policy = ActorCritic(input_dim, action_dim)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "# 메모리 클래스 정의\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "memory = Memory()\n",
    "\n",
    "# PPO 업데이트 함수 정의\n",
    "def ppo_update():\n",
    "    # 리스트를 텐서로 변환\n",
    "    states = torch.tensor(memory.states, dtype=torch.float32)\n",
    "    actions = torch.tensor(memory.actions, dtype=torch.int64)\n",
    "    old_logprobs = torch.tensor(memory.logprobs, dtype=torch.float32)\n",
    "    rewards = memory.rewards\n",
    "    is_terminals = memory.is_terminals\n",
    "\n",
    "    # 리턴 계산\n",
    "    returns = []\n",
    "    discounted_reward = 0\n",
    "    for reward, is_terminal in zip(reversed(rewards), reversed(is_terminals)):\n",
    "        if is_terminal:\n",
    "            discounted_reward = 0\n",
    "        discounted_reward = reward + (gamma * discounted_reward)\n",
    "        returns.insert(0, discounted_reward)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # 어드밴티지 계산\n",
    "    with torch.no_grad():\n",
    "        _, state_values = policy.forward(states)\n",
    "        advantages = returns - state_values.squeeze(-1)\n",
    "\n",
    "    # 정책 업데이트\n",
    "    for _ in range(epochs):\n",
    "        logprobs, state_values, dist_entropy = policy.evaluate(states, actions)\n",
    "        ratios = torch.exp(logprobs - old_logprobs)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "        loss = -torch.min(surr1, surr2) + 0.5 * advantages.pow(2) - entropy_coef * dist_entropy  # 엔트로피 가중치 증가\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        # 그레이디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 값 출력\n",
    "        print(f\"Loss: {loss.mean().item():.4f}\")\n",
    "\n",
    "# 학습 루프\n",
    "max_episodes = 10  # 에피소드 수를 줄여 빠른 테스트\n",
    "update_interval = 1\n",
    "\n",
    "# 행동 분포 추적을 위한 리스트\n",
    "action_counts = []\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action, action_logprob = policy.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # 메모리에 데이터 저장\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(action_logprob.item())\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "        state = next_state\n",
    "        episode_actions.append(action)\n",
    "\n",
    "    # 정책 업데이트 및 메모리 초기화\n",
    "    ppo_update()\n",
    "    memory.clear()\n",
    "\n",
    "    # 행동 분포 추적\n",
    "    action_counts.append(np.bincount(episode_actions, minlength=3))\n",
    "\n",
    "    # 에피소드별 성과 시각화\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # 포트폴리오 가치 변화 시각화\n",
    "    plt.subplot(5, 1, 1)\n",
    "    plt.plot(env.asset_value_history)\n",
    "    plt.title(f'Episode {episode+1} - Asset Value Over Time')\n",
    "    plt.ylabel('Asset Value')\n",
    "\n",
    "    # 포지션 변화 시각화\n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.plot(env.position_history)\n",
    "    plt.title('Position Over Time')\n",
    "    plt.ylabel('Position')\n",
    "\n",
    "    # 주가 변화 시각화\n",
    "    plt.subplot(5, 1, 3)\n",
    "    plt.plot(env.price_history)\n",
    "    plt.title('Price Over Time')\n",
    "    plt.ylabel('Price')\n",
    "\n",
    "    # 행동 시각화\n",
    "    plt.subplot(5, 1, 4)\n",
    "    plt.plot(env.action_history)\n",
    "    plt.title('Actions Over Time')\n",
    "    plt.ylabel('Action')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.yticks([0, 1, 2], ['Sell', 'Hold', 'Buy'])\n",
    "\n",
    "    # 행동 분포 시각화\n",
    "    plt.subplot(5, 1, 5)\n",
    "    counts = np.array(action_counts).sum(axis=0)\n",
    "    plt.bar(['Sell', 'Hold', 'Buy'], counts)\n",
    "    plt.title('Action Distribution')\n",
    "    plt.ylabel('Counts')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Episode {episode+1} completed. Final Asset Value: {env.asset_value_history[-1]:.2f}\")\n",
    "\n",
    "# 전체 행동 분포 시각화\n",
    "total_counts = np.array(action_counts).sum(axis=0)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['Sell', 'Hold', 'Buy'], total_counts)\n",
    "plt.title('Total Action Distribution')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
