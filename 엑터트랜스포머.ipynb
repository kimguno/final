{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "c:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 142\u001b[0m\n\u001b[0;32m    139\u001b[0m states, actions, rewards \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 142\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    145\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(state)\n",
      "Cell \u001b[1;32mIn[1], line 96\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     95\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(action_probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     70\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 배치 차원 추가\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 수집 및 전처리\n",
    "def load_data(ticker):\n",
    "    data = yf.download(ticker, start=\"2020-01-01\", end=\"2023-01-01\")\n",
    "    data['SMA'] = data['Close'].rolling(window=20).mean()\n",
    "    data['Return'] = data['Close'].pct_change()\n",
    "    data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "data = load_data(\"AAPL\")\n",
    "\n",
    "# 환경 정의\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.data = data.reset_index()\n",
    "        self.current_step = 0\n",
    "        self.action_space = gym.spaces.Discrete(3)  # 매수, 매도, 보유\n",
    "        self.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = 1000  # 초기 잔고\n",
    "        self.stock_owned = 0  # 보유 주식 수\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        obs = self.data.iloc[self.current_step][['Open', 'High', 'Low', 'Close', 'Volume', 'Return']].values\n",
    "        return obs.astype(np.float32)  # 데이터 타입 변환\n",
    "\n",
    "    def step(self, action):\n",
    "        current_price = self.data.iloc[self.current_step]['Close']\n",
    "        reward = 0\n",
    "\n",
    "        if action == 0:  # 매수\n",
    "            if self.balance >= current_price:\n",
    "                self.stock_owned += 1\n",
    "                self.balance -= current_price\n",
    "        elif action == 1:  # 매도\n",
    "            if self.stock_owned > 0:\n",
    "                self.stock_owned -= 1\n",
    "                self.balance += current_price\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.data) - 1\n",
    "        next_state = self._next_observation()\n",
    "\n",
    "        # 보상: 잔고 변화에 따른 보상\n",
    "        reward = self.balance + self.stock_owned * current_price - 1000  # 초기 잔고 1000\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "# 트랜스포머 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, n_heads, n_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer = nn.Transformer(d_model=model_dim, nhead=n_heads, num_encoder_layers=n_layers)\n",
    "        self.fc = nn.Linear(model_dim, 3)  # 행동의 확률 분포\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # 배치 차원 추가\n",
    "        x = self.transformer(x)\n",
    "        return torch.softmax(self.fc(x[-1]), dim=-1)  # 마지막 출력을 사용\n",
    "\n",
    "# 엑터-크리틱 모델 정의\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)  # 상태의 가치 추정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# PPO 에이전트 구현\n",
    "class PPOAgent:\n",
    "    def __init__(self, actor, critic, clip_param=0.2, gamma=0.99, lr=3e-4):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.clip_param = clip_param\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = torch.optim.Adam(list(actor.parameters()) + list(critic.parameters()), lr=lr)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_probs = self.actor(state_tensor)\n",
    "        action = torch.multinomial(action_probs, num_samples=1)\n",
    "        return action.item()\n",
    "\n",
    "    def update_policy(self, states, actions, rewards, next_states):\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        actions_tensor = torch.LongTensor(actions)\n",
    "\n",
    "        values = self.critic(states_tensor).squeeze()\n",
    "        next_values = self.critic(torch.FloatTensor(next_states)).squeeze()\n",
    "\n",
    "        td_targets = rewards + self.gamma * next_values\n",
    "        advantages = td_targets - values\n",
    "\n",
    "        action_probs = self.actor(states_tensor)\n",
    "        action_log_probs = torch.log(action_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze())\n",
    "        ratio = action_log_probs - torch.log(action_probs.gather(1, actions_tensor.unsqueeze(1)).squeeze())\n",
    "\n",
    "        surrogate1 = ratio * advantages.detach()\n",
    "        surrogate2 = torch.clamp(ratio, 1 - self.clip_param, 1 + self.clip_param) * advantages.detach()\n",
    "        actor_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "        critic_loss = nn.functional.mse_loss(values, td_targets.detach())\n",
    "\n",
    "        loss = actor_loss + critic_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# 모델 초기화\n",
    "env = StockTradingEnv(data)\n",
    "actor = TransformerModel(input_dim=6, model_dim=64, n_heads=4, n_layers=2)\n",
    "critic = Critic(input_dim=6)\n",
    "agent = PPOAgent(actor, critic)\n",
    "\n",
    "# 학습 루프\n",
    "num_episodes = 1000\n",
    "rewards_list = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    states, actions, rewards = [], [], []\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "    agent.update_policy(states, actions, rewards, [next_state] * len(states))\n",
    "    rewards_list.append(sum(rewards))\n",
    "\n",
    "# 결과 플롯\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(rewards_list)\n",
    "plt.title('Total Rewards over Episodes')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 최종 잔고 및 손실률 계산\n",
    "final_balance = env.balance + env.stock_owned * env.data.iloc[env.current_step]['Close']\n",
    "initial_investment = 1000\n",
    "loss_percentage = ((initial_investment - final_balance) / initial_investment) * 100\n",
    "\n",
    "print(f\"최종 잔고: {final_balance:.2f}\")\n",
    "print(f\"손실률: {loss_percentage:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
