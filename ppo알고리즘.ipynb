{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./000660.csv',encoding='cp949')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>날짜</th>\n",
       "      <th>시간</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>거래량</th>\n",
       "      <th>code</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MA20</th>\n",
       "      <th>MA60</th>\n",
       "      <th>MA120</th>\n",
       "      <th>Upper_Band</th>\n",
       "      <th>Lower_Band</th>\n",
       "      <th>day_name</th>\n",
       "      <th>매도량</th>\n",
       "      <th>매수량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-19 09:01:00</td>\n",
       "      <td>20220819</td>\n",
       "      <td>901</td>\n",
       "      <td>97000</td>\n",
       "      <td>97000</td>\n",
       "      <td>96500</td>\n",
       "      <td>96800</td>\n",
       "      <td>99206</td>\n",
       "      <td>660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Friday</td>\n",
       "      <td>11919.0</td>\n",
       "      <td>87281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-19 09:02:00</td>\n",
       "      <td>20220819</td>\n",
       "      <td>902</td>\n",
       "      <td>96800</td>\n",
       "      <td>97200</td>\n",
       "      <td>96700</td>\n",
       "      <td>97000</td>\n",
       "      <td>55648</td>\n",
       "      <td>660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Friday</td>\n",
       "      <td>15861.0</td>\n",
       "      <td>39787.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-19 09:03:00</td>\n",
       "      <td>20220819</td>\n",
       "      <td>903</td>\n",
       "      <td>97100</td>\n",
       "      <td>97100</td>\n",
       "      <td>96200</td>\n",
       "      <td>96200</td>\n",
       "      <td>37596</td>\n",
       "      <td>660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Friday</td>\n",
       "      <td>17740.0</td>\n",
       "      <td>19856.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-19 09:04:00</td>\n",
       "      <td>20220819</td>\n",
       "      <td>904</td>\n",
       "      <td>96200</td>\n",
       "      <td>96400</td>\n",
       "      <td>96100</td>\n",
       "      <td>96300</td>\n",
       "      <td>28194</td>\n",
       "      <td>660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Friday</td>\n",
       "      <td>15239.0</td>\n",
       "      <td>12955.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-19 09:05:00</td>\n",
       "      <td>20220819</td>\n",
       "      <td>905</td>\n",
       "      <td>96300</td>\n",
       "      <td>96500</td>\n",
       "      <td>96100</td>\n",
       "      <td>96400</td>\n",
       "      <td>19232</td>\n",
       "      <td>660</td>\n",
       "      <td>96540.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Friday</td>\n",
       "      <td>8216.0</td>\n",
       "      <td>11016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186835</th>\n",
       "      <td>2024-08-19 15:16:00</td>\n",
       "      <td>20240819</td>\n",
       "      <td>1516</td>\n",
       "      <td>193100</td>\n",
       "      <td>193200</td>\n",
       "      <td>193100</td>\n",
       "      <td>193200</td>\n",
       "      <td>11360</td>\n",
       "      <td>660</td>\n",
       "      <td>193140.0</td>\n",
       "      <td>192995.0</td>\n",
       "      <td>193810.00</td>\n",
       "      <td>194504.17</td>\n",
       "      <td>193370.44</td>\n",
       "      <td>192619.56</td>\n",
       "      <td>Monday</td>\n",
       "      <td>7343.0</td>\n",
       "      <td>4017.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186836</th>\n",
       "      <td>2024-08-19 15:17:00</td>\n",
       "      <td>20240819</td>\n",
       "      <td>1517</td>\n",
       "      <td>193200</td>\n",
       "      <td>193200</td>\n",
       "      <td>192900</td>\n",
       "      <td>193000</td>\n",
       "      <td>32191</td>\n",
       "      <td>660</td>\n",
       "      <td>193120.0</td>\n",
       "      <td>193005.0</td>\n",
       "      <td>193778.33</td>\n",
       "      <td>194476.67</td>\n",
       "      <td>193369.05</td>\n",
       "      <td>192640.95</td>\n",
       "      <td>Monday</td>\n",
       "      <td>26959.0</td>\n",
       "      <td>5232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186837</th>\n",
       "      <td>2024-08-19 15:18:00</td>\n",
       "      <td>20240819</td>\n",
       "      <td>1518</td>\n",
       "      <td>192900</td>\n",
       "      <td>193000</td>\n",
       "      <td>192800</td>\n",
       "      <td>192800</td>\n",
       "      <td>9585</td>\n",
       "      <td>660</td>\n",
       "      <td>193040.0</td>\n",
       "      <td>193000.0</td>\n",
       "      <td>193746.67</td>\n",
       "      <td>194448.33</td>\n",
       "      <td>193372.77</td>\n",
       "      <td>192627.23</td>\n",
       "      <td>Monday</td>\n",
       "      <td>6745.0</td>\n",
       "      <td>2840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186838</th>\n",
       "      <td>2024-08-19 15:19:00</td>\n",
       "      <td>20240819</td>\n",
       "      <td>1519</td>\n",
       "      <td>192800</td>\n",
       "      <td>193100</td>\n",
       "      <td>192800</td>\n",
       "      <td>193000</td>\n",
       "      <td>16917</td>\n",
       "      <td>660</td>\n",
       "      <td>193020.0</td>\n",
       "      <td>193000.0</td>\n",
       "      <td>193713.33</td>\n",
       "      <td>194421.67</td>\n",
       "      <td>193372.77</td>\n",
       "      <td>192627.23</td>\n",
       "      <td>Monday</td>\n",
       "      <td>4646.0</td>\n",
       "      <td>12271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186839</th>\n",
       "      <td>2024-08-19 15:20:00</td>\n",
       "      <td>20240819</td>\n",
       "      <td>1520</td>\n",
       "      <td>193100</td>\n",
       "      <td>193300</td>\n",
       "      <td>193000</td>\n",
       "      <td>193200</td>\n",
       "      <td>16560</td>\n",
       "      <td>660</td>\n",
       "      <td>193040.0</td>\n",
       "      <td>193035.0</td>\n",
       "      <td>193685.00</td>\n",
       "      <td>194398.33</td>\n",
       "      <td>193334.30</td>\n",
       "      <td>192735.70</td>\n",
       "      <td>Monday</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>10739.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186840 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Unnamed: 0        날짜    시간      시가      고가      저가      종가  \\\n",
       "0       2022-08-19 09:01:00  20220819   901   97000   97000   96500   96800   \n",
       "1       2022-08-19 09:02:00  20220819   902   96800   97200   96700   97000   \n",
       "2       2022-08-19 09:03:00  20220819   903   97100   97100   96200   96200   \n",
       "3       2022-08-19 09:04:00  20220819   904   96200   96400   96100   96300   \n",
       "4       2022-08-19 09:05:00  20220819   905   96300   96500   96100   96400   \n",
       "...                     ...       ...   ...     ...     ...     ...     ...   \n",
       "186835  2024-08-19 15:16:00  20240819  1516  193100  193200  193100  193200   \n",
       "186836  2024-08-19 15:17:00  20240819  1517  193200  193200  192900  193000   \n",
       "186837  2024-08-19 15:18:00  20240819  1518  192900  193000  192800  192800   \n",
       "186838  2024-08-19 15:19:00  20240819  1519  192800  193100  192800  193000   \n",
       "186839  2024-08-19 15:20:00  20240819  1520  193100  193300  193000  193200   \n",
       "\n",
       "          거래량  code       MA5      MA20       MA60      MA120  Upper_Band  \\\n",
       "0       99206   660       NaN       NaN        NaN        NaN         NaN   \n",
       "1       55648   660       NaN       NaN        NaN        NaN         NaN   \n",
       "2       37596   660       NaN       NaN        NaN        NaN         NaN   \n",
       "3       28194   660       NaN       NaN        NaN        NaN         NaN   \n",
       "4       19232   660   96540.0       NaN        NaN        NaN         NaN   \n",
       "...       ...   ...       ...       ...        ...        ...         ...   \n",
       "186835  11360   660  193140.0  192995.0  193810.00  194504.17   193370.44   \n",
       "186836  32191   660  193120.0  193005.0  193778.33  194476.67   193369.05   \n",
       "186837   9585   660  193040.0  193000.0  193746.67  194448.33   193372.77   \n",
       "186838  16917   660  193020.0  193000.0  193713.33  194421.67   193372.77   \n",
       "186839  16560   660  193040.0  193035.0  193685.00  194398.33   193334.30   \n",
       "\n",
       "        Lower_Band day_name      매도량      매수량  \n",
       "0              NaN   Friday  11919.0  87281.0  \n",
       "1              NaN   Friday  15861.0  39787.0  \n",
       "2              NaN   Friday  17740.0  19856.0  \n",
       "3              NaN   Friday  15239.0  12955.0  \n",
       "4              NaN   Friday   8216.0  11016.0  \n",
       "...            ...      ...      ...      ...  \n",
       "186835   192619.56   Monday   7343.0   4017.0  \n",
       "186836   192640.95   Monday  26959.0   5232.0  \n",
       "186837   192627.23   Monday   6745.0   2840.0  \n",
       "186838   192627.23   Monday   4646.0  12271.0  \n",
       "186839   192735.70   Monday   5821.0  10739.0  \n",
       "\n",
       "[186840 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# 데이터 로드 및 기술적 지표 계산\n",
    "# df = yf.download('AAPL', start='2017-01-01', end='2020-12-31', progress=False)\n",
    "df = pd.read_csv('./000660.csv',encoding='cp949')\n",
    "df = df.rename(columns={'종가': 'Close'})\n",
    "\n",
    "df['MA10'] = df['Close'].rolling(window=10).mean()\n",
    "df['MA50'] = df['Close'].rolling(window=50).mean()\n",
    "df['RSI'] = 100 - (100 / (1 + df['Close'].diff().apply(lambda x: np.maximum(x, 0))\n",
    "                          .rolling(window=14).mean() / df['Close'].diff()\n",
    "                          .apply(lambda x: np.abs(np.minimum(x, 0)))\n",
    "                          .rolling(window=14).mean()))\n",
    "df = df.dropna().reset_index()\n",
    "\n",
    "# 입력 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "feature_cols = ['Close', 'MA10', 'MA50', 'RSI']\n",
    "scaler.fit(df[feature_cols])\n",
    "\n",
    "# 주식 트레이딩 환경 정의\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df, scaler):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.scaler = scaler\n",
    "        self.max_steps = len(df) - 1\n",
    "        self.current_step = 0\n",
    "\n",
    "        # 상태 공간: [현재 가격, MA10, MA50, RSI, 보유 여부]\n",
    "        self.action_space = spaces.Discrete(3)  # 매도(0), 보유(1), 매수(2)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # 초기 자본금 및 포지션\n",
    "        self.initial_balance = 10000000\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0  # 0: 현금, 1: 주식 보유\n",
    "        self.asset_value = self.initial_balance  # 포트폴리오 총 가치\n",
    "\n",
    "        # 추적을 위한 리스트\n",
    "        self.balance_history = []\n",
    "        self.asset_value_history = []\n",
    "        self.position_history = []\n",
    "        self.action_history = []\n",
    "        self.price_history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0\n",
    "        self.asset_value = self.initial_balance\n",
    "        self.current_step = 0\n",
    "\n",
    "        # 히스토리 초기화\n",
    "        self.balance_history = [self.balance]\n",
    "        self.asset_value_history = [self.asset_value]\n",
    "        self.position_history = [self.position]\n",
    "        self.action_history = []\n",
    "        self.price_history = [self.df.loc[self.current_step, 'Close']]\n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        raw_state = self.df.loc[self.current_step, ['Close', 'MA10', 'MA50', 'RSI']]\n",
    "        # Series를 DataFrame으로 변환\n",
    "        raw_state_df = pd.DataFrame([raw_state])\n",
    "        # scaler.transform에 DataFrame을 입력\n",
    "        scaled_state = self.scaler.transform(raw_state_df)[0]\n",
    "        frame = np.append(scaled_state, self.position)\n",
    "        return frame.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        price = self.df.loc[self.current_step, 'Close']\n",
    "\n",
    "        # 이전 포트폴리오 가치\n",
    "        prev_asset_value = self.asset_value\n",
    "\n",
    "        # 행동에 따른 포트폴리오 업데이트\n",
    "        if action == 0:  # 매도\n",
    "            if self.position == 1:\n",
    "                self.balance += price\n",
    "                self.position = 0\n",
    "        elif action == 1:  # 보유\n",
    "            pass\n",
    "        elif action == 2:  # 매수\n",
    "            if self.position == 0:\n",
    "                self.balance -= price\n",
    "                self.position = 1\n",
    "\n",
    "        # 포트폴리오 가치 계산\n",
    "        self.asset_value = self.balance + self.position * price\n",
    "\n",
    "        # 보유 비용 적용 (포지션을 보유할 때마다 일정 비용 부과)\n",
    "        holding_cost = 0\n",
    "        if self.position == 1:\n",
    "            holding_cost = 0.001 * price  # 가격의 0.1%를 보유 비용으로 설정\n",
    "            self.asset_value -= holding_cost\n",
    "\n",
    "        # 보상 계산: 포트폴리오 가치의 변화 (스케일링)\n",
    "        reward = (self.asset_value - prev_asset_value) * 10  # 보상 스케일링\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        # 히스토리 업데이트\n",
    "        self.balance_history.append(self.balance)\n",
    "        self.asset_value_history.append(self.asset_value)\n",
    "        self.position_history.append(self.position)\n",
    "        self.action_history.append(action)\n",
    "        self.price_history.append(price)\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# 환경 생성\n",
    "env = StockTradingEnv(df, scaler)\n",
    "\n",
    "# PPO를 위한 액터-크리틱 신경망 정의\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy_head = nn.Linear(128, action_dim)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "        \n",
    "        # 가중치 초기화\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        policy_logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return policy_logits, value\n",
    "\n",
    "    def act(self, state):\n",
    "        # state = torch.FloatTensor(state)\n",
    "        policy_logits, _ = self.forward(state)\n",
    "        dist = Categorical(logits=policy_logits)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        return action.item(), action_logprob\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        policy_logits, value = self.forward(state)\n",
    "        dist = Categorical(logits=policy_logits)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        return action_logprobs, value.squeeze(-1), dist_entropy\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 1e-3  # 학습률 증가\n",
    "gamma = 0.99\n",
    "epsilon = 0.1         # 클리핑 파라미터 감소\n",
    "epochs = 10\n",
    "entropy_coef = 0.05   # 엔트로피 가중치 증가\n",
    "\n",
    "# 정책 및 옵티마이저 초기화\n",
    "input_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "# policy = ActorCritic(input_dim, action_dim)\n",
    "policy = ActorCritic(input_dim, action_dim).to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "# 메모리 클래스 정의\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "memory = Memory()\n",
    "\n",
    "# PPO 업데이트 함수 정의\n",
    "def ppo_update():\n",
    "    # 리스트를 텐서로 변환\n",
    "    states = torch.tensor(memory.states, dtype=torch.float32).to(device)\n",
    "    actions = torch.tensor(memory.actions, dtype=torch.int64).to(device)\n",
    "    old_logprobs = torch.tensor(memory.logprobs, dtype=torch.float32).to(device)\n",
    "    rewards = memory.rewards\n",
    "    is_terminals = memory.is_terminals\n",
    "\n",
    "    # 리턴 계산\n",
    "    returns = []\n",
    "    discounted_reward = 0\n",
    "    for reward, is_terminal in zip(reversed(rewards), reversed(is_terminals)):\n",
    "        if is_terminal:\n",
    "            discounted_reward = 0\n",
    "        discounted_reward = reward + (gamma * discounted_reward)\n",
    "        returns.insert(0, discounted_reward)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "\n",
    "    # 어드밴티지 계산\n",
    "    with torch.no_grad():\n",
    "        _, state_values = policy.forward(states)\n",
    "        advantages = returns - state_values.squeeze(-1)\n",
    "\n",
    "    # 정책 업데이트\n",
    "    for _ in range(epochs):\n",
    "        logprobs, state_values, dist_entropy = policy.evaluate(states, actions)\n",
    "        ratios = torch.exp(logprobs - old_logprobs)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "        loss = -torch.min(surr1, surr2) + 0.5 * advantages.pow(2) - entropy_coef * dist_entropy  # 엔트로피 가중치 증가\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        # 그레이디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 값 출력\n",
    "        print(f\"Loss: {loss.mean().item():.4f}\")\n",
    "\n",
    "# 학습 루프\n",
    "max_episodes = 10  # 에피소드 수를 줄여 빠른 테스트\n",
    "update_interval = 1\n",
    "\n",
    "# 행동 분포 추적을 위한 리스트\n",
    "action_counts = []\n",
    "best_asset_value = 0\n",
    "best_model_path = 'best_actor_critic_model.pth'\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    print(f'에피소드 {episode+1} 시작!!!')\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        state_tensor = torch.FloatTensor(state).to(device)\n",
    "        action, action_logprob = policy.act(state_tensor)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # 메모리에 데이터 저장\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(action_logprob.item())\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "        state = next_state\n",
    "        episode_actions.append(action)\n",
    "\n",
    "    # 정책 업데이트 및 메모리 초기화\n",
    "    ppo_update()\n",
    "    memory.clear()\n",
    "\n",
    "    # 행동 분포 추적\n",
    "    action_counts.append(np.bincount(episode_actions, minlength=3))\n",
    "\n",
    "    # 에피소드별 성과 시각화\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # 포트폴리오 가치 변화 시각화\n",
    "    plt.subplot(5, 1, 1)\n",
    "    plt.plot(env.asset_value_history)\n",
    "    plt.title(f'Episode {episode+1} - Asset Value Over Time')\n",
    "    plt.ylabel('Asset Value')\n",
    "\n",
    "    # 포지션 변화 시각화\n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.plot(env.position_history)\n",
    "    plt.title('Position Over Time')\n",
    "    plt.ylabel('Position')\n",
    "\n",
    "    # 주가 변화 시각화\n",
    "    plt.subplot(5, 1, 3)\n",
    "    plt.plot(env.price_history)\n",
    "    plt.title('Price Over Time')\n",
    "    plt.ylabel('Price')\n",
    "\n",
    "    # 행동 시각화\n",
    "    plt.subplot(5, 1, 4)\n",
    "    plt.plot(env.action_history)\n",
    "    plt.title('Actions Over Time')\n",
    "    plt.ylabel('Action')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.yticks([0, 1, 2], ['Sell', 'Hold', 'Buy'])\n",
    "\n",
    "    # 행동 분포 시각화\n",
    "    plt.subplot(5, 1, 5)\n",
    "    counts = np.array(action_counts).sum(axis=0)\n",
    "    plt.bar(['Sell', 'Hold', 'Buy'], counts)\n",
    "    plt.title('Action Distribution')\n",
    "    plt.ylabel('Counts')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Episode {episode+1} completed. Final Asset Value: {env.asset_value_history[-1]:.2f}\")\n",
    "    # 베스트 모델 저장 로직\n",
    "    if final_asset_value > best_asset_value:\n",
    "        best_asset_value = final_asset_value\n",
    "        torch.save(policy.state_dict(), best_model_path)  # 모델 저장\n",
    "        print(f\"Best model saved with asset value: {best_asset_value:.2f}\")\n",
    "\n",
    "# 전체 행동 분포 시각화\n",
    "total_counts = np.array(action_counts).sum(axis=0)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['Sell', 'Hold', 'Buy'], total_counts)\n",
    "plt.title('Total Action Distribution')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
