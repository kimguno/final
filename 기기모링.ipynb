{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode1 시작\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'tgt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 280\u001b[0m\n\u001b[0;32m    278\u001b[0m episode_actions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m--> 280\u001b[0m     action, action_logprob \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# 메모리에 데이터 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 181\u001b[0m, in \u001b[0;36mActorCritic.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m    180\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 181\u001b[0m     policy_logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     dist \u001b[38;5;241m=\u001b[39m Categorical(logits\u001b[38;5;241m=\u001b[39mpolicy_logits)\n\u001b[0;32m    183\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "Cell \u001b[1;32mIn[1], line 174\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 174\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m     policy_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_head(x)\n\u001b[0;32m    176\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_head(x)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 154\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(x)  \u001b[38;5;66;03m# 포지셔닝 인코딩 추가\u001b[39;00m\n\u001b[0;32m    153\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 배치 차원 추가\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\dl-dev\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'tgt'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터 로드 및 기술적 지표 계산\n",
    "df = pd.read_csv('./000660.csv', encoding='cp949')\n",
    "df.rename(columns={'종가':'Close'}, inplace=True)\n",
    "df['MA10'] = df['Close'].rolling(window=10).mean()\n",
    "df['MA50'] = df['Close'].rolling(window=50).mean()\n",
    "df['RSI'] = 100 - (100 / (1 + df['Close'].diff().apply(lambda x: np.maximum(x, 0))\n",
    "                          .rolling(window=14).mean() / df['Close'].diff()\n",
    "                          .apply(lambda x: np.abs(np.minimum(x, 0)))\n",
    "                          .rolling(window=14).mean()))\n",
    "df = df.dropna().reset_index()\n",
    "\n",
    "# 입력 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "feature_cols = ['Close', 'MA10', 'MA50', 'RSI']\n",
    "scaler.fit(df[feature_cols])\n",
    "\n",
    "# 주식 트레이딩 환경 정의\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df, scaler):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.scaler = scaler\n",
    "        self.max_steps = len(df) - 1\n",
    "        self.current_step = 0\n",
    "\n",
    "        # 상태 공간: [현재 가격, MA10, MA50, RSI, 보유 여부]\n",
    "        self.action_space = spaces.Discrete(3)  # 매도(0), 보유(1), 매수(2)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # 초기 자본금 및 포지션\n",
    "        self.initial_balance = 10000000\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0  # 0: 현금, 1: 주식 보유\n",
    "        self.asset_value = self.initial_balance  # 포트폴리오 총 가치\n",
    "\n",
    "        # 추적을 위한 리스트\n",
    "        self.balance_history = []\n",
    "        self.asset_value_history = []\n",
    "        self.position_history = []\n",
    "        self.action_history = []\n",
    "        self.price_history = []\n",
    "\n",
    "    def reset(self):\n",
    "        self.balance = self.initial_balance\n",
    "        self.position = 0\n",
    "        self.asset_value = self.initial_balance\n",
    "        self.current_step = 0\n",
    "\n",
    "        # 히스토리 초기화\n",
    "        self.balance_history = [self.balance]\n",
    "        self.asset_value_history = [self.asset_value]\n",
    "        self.position_history = [self.position]\n",
    "        self.action_history = []\n",
    "        self.price_history = [self.df.loc[self.current_step, 'Close']]\n",
    "\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        raw_state = self.df.loc[self.current_step, ['Close', 'MA10', 'MA50', 'RSI']]\n",
    "        raw_state_df = pd.DataFrame([raw_state])\n",
    "        scaled_state = self.scaler.transform(raw_state_df)[0]\n",
    "        frame = np.append(scaled_state, self.position)\n",
    "        return frame.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        price = self.df.loc[self.current_step, 'Close']\n",
    "\n",
    "        # 이전 포트폴리오 가치\n",
    "        prev_asset_value = self.asset_value\n",
    "\n",
    "        # 행동에 따른 포트폴리오 업데이트\n",
    "        if action == 0:  # 매도\n",
    "            if self.position == 1:\n",
    "                self.balance += price\n",
    "                self.position = 0\n",
    "        elif action == 1:  # 보유\n",
    "            pass\n",
    "        elif action == 2:  # 매수\n",
    "            if self.position == 0:\n",
    "                self.balance -= price\n",
    "                self.position = 1\n",
    "\n",
    "        # 포트폴리오 가치 계산\n",
    "        self.asset_value = self.balance + self.position * price\n",
    "\n",
    "        # 보유 비용 적용\n",
    "        holding_cost = 0\n",
    "        if self.position == 1:\n",
    "            holding_cost = 0.001 * price\n",
    "            self.asset_value -= holding_cost\n",
    "\n",
    "        # 보상 계산\n",
    "        reward = (self.asset_value - prev_asset_value) * 10\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        # 히스토리 업데이트\n",
    "        self.balance_history.append(self.balance)\n",
    "        self.asset_value_history.append(self.asset_value)\n",
    "        self.position_history.append(self.position)\n",
    "        self.action_history.append(action)\n",
    "        self.price_history.append(price)\n",
    "\n",
    "        obs = self._next_observation()\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "# 환경 생성\n",
    "env = StockTradingEnv(df, scaler)\n",
    "\n",
    "# 포지셔닝 인코딩 정의\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model))\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)  # 배치 차원을 추가\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.encoding[:, :x.size(1), :]\n",
    "\n",
    "# 트랜스포머 모델 정의 (포지셔닝 인코딩 포함)\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim=128, n_heads=4, n_layers=2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.positional_encoding = PositionalEncoding(model_dim)\n",
    "        self.transformer = nn.Transformer(d_model=model_dim, nhead=n_heads, num_encoder_layers=n_layers)\n",
    "        self.fc = nn.Linear(model_dim, 3)  # 행동의 확률 분포\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)  # 포지셔닝 인코딩 추가\n",
    "        x = x.unsqueeze(1)  # 배치 차원 추가\n",
    "        x = self.transformer(x)\n",
    "        return torch.softmax(self.fc(x[-1]), dim=-1)  # 마지막 출력을 사용\n",
    "\n",
    "# PPO를 위한 액터-크리틱 신경망 정의\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.model = TransformerModel(input_dim)\n",
    "        self.policy_head = nn.Linear(128, action_dim)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        policy_logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return policy_logits, value\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        policy_logits, _ = self.forward(state)\n",
    "        dist = Categorical(logits=policy_logits)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        return action.item(), action_logprob\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        state = torch.FloatTensor(state)\n",
    "        policy_logits, value = self.forward(state)\n",
    "        dist = Categorical(logits=policy_logits)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        return action_logprobs, value.squeeze(-1), dist_entropy\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "epochs = 10\n",
    "entropy_coef = 0.05\n",
    "\n",
    "# 정책 및 옵티마이저 초기화\n",
    "input_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "policy = ActorCritic(input_dim, action_dim)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "# 메모리 클래스 정의\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "memory = Memory()\n",
    "\n",
    "# PPO 업데이트 함수 정의\n",
    "def ppo_update():\n",
    "    # 리스트를 텐서로 변환\n",
    "    states = torch.tensor(memory.states, dtype=torch.float32)\n",
    "    actions = torch.tensor(memory.actions, dtype=torch.int64)\n",
    "    old_logprobs = torch.tensor(memory.logprobs, dtype=torch.float32)\n",
    "    rewards = memory.rewards\n",
    "    is_terminals = memory.is_terminals\n",
    "\n",
    "    # 리턴 계산\n",
    "    returns = []\n",
    "    discounted_reward = 0\n",
    "    for reward, is_terminal in zip(reversed(rewards), reversed(is_terminals)):\n",
    "        if is_terminal:\n",
    "            discounted_reward = 0\n",
    "        discounted_reward = reward + (gamma * discounted_reward)\n",
    "        returns.insert(0, discounted_reward)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "\n",
    "    # 어드밴티지 계산\n",
    "    with torch.no_grad():\n",
    "        _, state_values = policy.forward(states)\n",
    "        advantages = returns - state_values.squeeze(-1)\n",
    "\n",
    "    # 정책 업데이트\n",
    "    for _ in range(epochs):\n",
    "        logprobs, state_values, dist_entropy = policy.evaluate(states, actions)\n",
    "        ratios = torch.exp(logprobs - old_logprobs)\n",
    "        surr1 = ratios * advantages\n",
    "        surr2 = torch.clamp(ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "        loss = -torch.min(surr1, surr2) + 0.5 * advantages.pow(2) - entropy_coef * dist_entropy  # 엔트로피 가중치 증가\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        # 그레이디언트 클리핑\n",
    "        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실 값 출력\n",
    "        print(f\"Loss: {loss.mean().item():.4f}\")\n",
    "\n",
    "# 학습 루프\n",
    "max_episodes = 10  # 에피소드 수를 줄여 빠른 테스트\n",
    "update_interval = 1\n",
    "\n",
    "# 행동 분포 추적을 위한 리스트\n",
    "action_counts = []\n",
    "\n",
    "for episode in range(max_episodes):\n",
    "    print(f'episode{episode+1} 시작')\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_actions = []\n",
    "    while not done:\n",
    "        action, action_logprob = policy.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # 메모리에 데이터 저장\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(action_logprob.item())\n",
    "        memory.rewards.append(reward)\n",
    "        memory.is_terminals.append(done)\n",
    "        state = next_state\n",
    "        episode_actions.append(action)\n",
    "\n",
    "    # 정책 업데이트 및 메모리 초기화\n",
    "    ppo_update()\n",
    "    memory.clear()\n",
    "\n",
    "    # 행동 분포 추적\n",
    "    action_counts.append(np.bincount(episode_actions, minlength=3))\n",
    "\n",
    "    # 에피소드별 성과 시각화\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # 포트폴리오 가치 변화 시각화\n",
    "    plt.subplot(5, 1, 1)\n",
    "    plt.plot(env.asset_value_history)\n",
    "    plt.title(f'Episode {episode+1} - Asset Value Over Time')\n",
    "    plt.ylabel('Asset Value')\n",
    "\n",
    "    # 포지션 변화 시각화\n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.plot(env.position_history)\n",
    "    plt.title('Position Over Time')\n",
    "    plt.ylabel('Position')\n",
    "\n",
    "    # 주가 변화 시각화\n",
    "    plt.subplot(5, 1, 3)\n",
    "    plt.plot(env.price_history)\n",
    "    plt.title('Price Over Time')\n",
    "    plt.ylabel('Price')\n",
    "\n",
    "    # 행동 시각화\n",
    "    plt.subplot(5, 1, 4)\n",
    "    plt.plot(env.action_history)\n",
    "    plt.title('Actions Over Time')\n",
    "    plt.ylabel('Action')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.yticks([0, 1, 2], ['Sell', 'Hold', 'Buy'])\n",
    "\n",
    "    # 행동 분포 시각화\n",
    "    plt.subplot(5, 1, 5)\n",
    "    counts = np.array(action_counts).sum(axis=0)\n",
    "    plt.bar(['Sell', 'Hold', 'Buy'], counts)\n",
    "    plt.title('Action Distribution')\n",
    "    plt.ylabel('Counts')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Episode {episode+1} completed. Final Asset Value: {env.asset_value_history[-1]:.2f}\")\n",
    "\n",
    "# 전체 행동 분포 시각화\n",
    "total_counts = np.array(action_counts).sum(axis=0)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(['Sell', 'Hold', 'Buy'], total_counts)\n",
    "plt.title('Total Action Distribution')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "# PPO 알고리즘 적용한 트레이딩 예제"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
